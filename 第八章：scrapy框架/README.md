scrapy框架

- 什么是框架？
    - 就是一个继承了很多功能，并且具有很强通用性的一个模板。
    
- 如何学习框架？
    - 专门学习框架封装的各种功能的详细用法。

- 什么是scrapy
    - 爬虫中封装好的一个*明星*框架。功能：高性能的持久化存储，异步的数据下载，高性能的数据解析，
        分布式
      
- scrapy框架的基本使用
    - 0.环境的安装：
        
    - 1.创建一个工程：scrapy startproject xxxPro
    - 2.cd xxxPro  
    - 3.在spiders子目录中创建一个爬虫文件
        - scrapy genspider spiderName www.xxx.cn
    - 4.执行工程：
        - scrapy crawl spiderName
        - scrapy crawl spiderName --nolog (可只看输出结果，但程序出错会有问题)
  
- scrapy数据解析

- scrapy持久化存储：
    - 基于终端指令：
        - 要求：只可以将parse方法的返回值存储到本地的文本文件中
        - 注意：持久化存储的文本文件格式只能为('json', 'jsonlines', 'jl', 'csv', 'xml', 'marshal', 'pickle'
        - 指令：scrapy crawl spiderName -o filePath
        - 好处：简洁高效便捷
        - 缺点：局限性比较强(数据只可以存储到指定后缀的文本文件中)
        
    - 基于管道pipeline：
        - 编码流程：
            - 数据解析
            - 在item类中定义相关的属性  
            - 将解析的数据封装存储到item类型的对象
            - 将item类型的对象提交给管道进行持久化存储的操作
            - 在管道类的process_item中要将其接收到的item对象中存储的数据进行持久化存储操作
            - 在配置文件中开启管道
        - 好处：
            - 通用性强
        - 缺点：
            - 略微繁琐
    
    - 面试题：将爬取到的数据一份存储到本地，一份存储到数据库，如何实现？
        - 管道文件中的一个管道类对应的是将数据存储到一种平台
        - 爬虫文件提交的item只会给管道文件中第一个被执行的管道类接受
        - process_item 中的 return item 语句表示将item传递给下一个即将被执行的管道类
    
- 基于Spider的全站数据爬取
    - 全站数据爬取，就是将网站中某板块下的全部页码对应的页面数据爬取
    - 需求：爬取校花网中的照片的名称
    - 实现方式：
        - 将所有的页面的url添加到start_urls列表（不推荐）
        - 自行手动进行请求发送(推荐)
            - 手动请求发送：
                - yield scrapy.Requests(url, callback): callback用作于数据解析
    
- 五大核心组件
    - 引擎（Scrapy）
        - 用来处理整个系统的数据流处理，触发事务（框架核心）
    - 调度器（Scheduler）
        - 用来接收引擎发过来的请求，压入队列中，并在引擎再次请求的时候返回，可以想象成一个URL（抓取网页的
            网址或者说是链接）的优先队列，由他来决定下一个要抓去的网址是什么，同时去除重复的网址
    - 下载器（Downloader）
        - 用于下载网页内容，并将网页内容返回给蜘蛛（Scrapy下载器是建立在twisted这个高效的异步模型上的）
    - 爬虫（Spiders）
        - 爬虫主要干活的，用于从特定的网页中提取自己需要的信息，即所谓的实体（item），用户也可以从中提取出
            链接，让Scrapy继续抓取下一个页面
    - 项目管道（Pipeline）
        - 负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息，当页面
            被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据
          
- 请求传参
    - 使用场景：如果爬取解析的数据不在同一张页面中。（深度爬取）
    - 需求：爬取boss直聘的岗位名称，岗位描述
    






















